{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08f5510-1e26-4b95-b62e-22b79a61cd2f",
   "metadata": {},
   "source": [
    "# ch 19_2 huggingface tokenizers\n",
    "\n",
    "이번 챕터에서는 huggingface에서 제공하는 tokenizers 라이브러리를 이용해서 직접 corpus 데이터 셋을 가지고 subword tokenizer를 학습시켜 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f02dc-81d2-42d6-9431-8adaba8a0a97",
   "metadata": {},
   "source": [
    "## 데이터 셋 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9efdff-2f2c-47f2-98b5-f23bbf22acb1",
   "metadata": {},
   "source": [
    "### 네이버 영화 리뷰 데이터 셋\n",
    "\n",
    "네이버 영화 리뷰 데이터 셋으로 간단히 학습을 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19586242-f6c9-4b1a-9cde-f980ffd459da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4a77185-e75a-416e-8b79-baf5a791b39b",
   "metadata": {},
   "source": [
    "### 데이터 셋 전처리\n",
    "\n",
    "결측치를 제거하고, 특수문자나 한자를 제거해주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f4a4c-788e-4ae5-b70d-c4db4c6345b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfcfb1b1-092c-417a-b159-2e180b237008",
   "metadata": {},
   "source": [
    "## huggingface tokenizers\n",
    "\n",
    "huggingface는 AI 스타트업으로 오픈 소스 라이브러리로 유명합니다. 특히 NLP 분야에서는 huggingface에서 제공하는 트랜스포머 모델을 사용하는 것이 거의 표준으로 자리잡았습니다. 주요 라이브러리는 아래와 같습니다. \n",
    "\n",
    "- transformers: 트랜스포머 기본 모델과 이를 응용한 NLP 모델들을 제공\n",
    "- tokenizers: subword tokenizer 제공\n",
    "\n",
    "huggingface의 tokenizers는 subword tokenizer의 구현체입니다. 이를 사용하여 tokenizer를 학습시켜 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b24984-6965-47eb-a0a3-6e7d080ec5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0212e38a-461e-46d7-8d6b-96d42dc4b9f3",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "WordPiece 기반의 subword tokenizer를 만들어보겠습니다.  먼저 tokenizer를 학습시키기 위한 trainer를 만들어줍니다. 이 때, special tokens를 넣어주어야 하는데, 각 토큰의 의미는 다음과 같습니다.\n",
    "\n",
    "- [PAD]: padding의 약자로 문장 간에 길이를 맞춰주기 위해 일부러 채워넣은 토큰을 의미합니다.\n",
    "- [UNK]: unknown의 약자로 인식하지 못한 토큰을 나타냅니다.\n",
    "- [SOS]: start of sentence의 약자로 문장의 시작을 표시해줍니다.\n",
    "- [EOS]: end of sentence의 약자로 문장의 끝을 표시해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3aa045-b350-431b-853a-2c96be125545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c838c82d-a95b-4b0c-a14f-375d3d307577",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "이제 tokenizer 객체를 만들어주고 trainer를 이용해서 학습시켜 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97ea1b-14a2-4745-8bd0-b099c252d851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4dd1491-9711-4648-97f2-002325d64b7a",
   "metadata": {},
   "source": [
    "## tokenizer 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ede0e-15e0-4450-99c4-350410db649a",
   "metadata": {},
   "source": [
    "### vocab 확인\n",
    "\n",
    "먼저 tokenizer vocab에 어떤 토큰들이 추가되었는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd4e041-48a0-47ea-b53e-fa1b593e1da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "620f5b96-dbb7-48d2-86b7-5add687a7f21",
   "metadata": {},
   "source": [
    "4000 ~ 5000개까지는 초기 기본 토큰들로 채워져 있고, 그 뒤로는 함께 자주 등장하는 글자끼리 묶인 토큰들을 확인할 수 있습니다. 앞에 ##이 붙은 토큰들은 단어의 시작 지점이 아닌 위치에 등장하는 토큰들입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf29bd92-2cbc-4d97-ad0b-bbdaafda9526",
   "metadata": {},
   "source": [
    "### 샘플 토큰화\n",
    "\n",
    "예시 문장들을 토큰화 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba1d665-b5cb-4b33-8727-26bed6ffdb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee7804e2-c961-45db-b4d8-76d17d42efa5",
   "metadata": {},
   "source": [
    "### 저장\n",
    "\n",
    "잘 학습된 것을 확인했다면 파일에 저장하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27569947-c081-45e1-8986-86b9e07028cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b4a5e3a-7625-4eb5-a9bd-5036ef7fb759",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이번 챕터에서는 huggingface에서 제공하는 tokenizers 라이브러리를 이용해서 직접 subword tokenizer를 만들어보았습니다. subword tokenizer는 활용도가 매우 높으니, 사용법을 잘 기억해주시기 바랍니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
