{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08f5510-1e26-4b95-b62e-22b79a61cd2f",
   "metadata": {},
   "source": [
    "# ch 19_2 huggingface tokenizers\n",
    "\n",
    "이번 챕터에서는 huggingface에서 제공하는 tokenizers 라이브러리를 이용해서 직접 corpus 데이터 셋을 가지고 subword tokenizer를 학습시켜 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f02dc-81d2-42d6-9431-8adaba8a0a97",
   "metadata": {},
   "source": [
    "## 데이터 셋 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9efdff-2f2c-47f2-98b5-f23bbf22acb1",
   "metadata": {},
   "source": [
    "### 네이버 영화 리뷰 데이터 셋\n",
    "\n",
    "네이버 영화 리뷰 데이터 셋으로 간단히 학습을 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19586242-f6c9-4b1a-9cde-f980ffd459da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"./data/naver_reviews_train.csv\")\n",
    "val_df = pd.read_csv(\"./data/naver_reviews_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25758838-779e-40f9-9a83-085f0c3a1189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>평점이 너무 낮아서...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 document  label\n",
       "0                                     아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1                       흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2                                       너무재밓었다그래서보는것을추천한다      0\n",
       "3                           교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4       사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
       "...                                                   ...    ...\n",
       "149995                                인간이 문제지.. 소는 뭔죄인가..      0\n",
       "149996                                      평점이 너무 낮아서...      1\n",
       "149997                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
       "149998                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
       "149999                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[150000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a77185-e75a-416e-8b79-baf5a791b39b",
   "metadata": {},
   "source": [
    "### 데이터 셋 전처리\n",
    "\n",
    "결측치를 제거하고, 특수문자나 한자를 제거해주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b01f4a4c-788e-4ae5-b70d-c4db4c6345b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document    5\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec3ef19-aaf3-4f1e-b479-bc288387b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b9d401-8308-42f9-b5c4-708d4925d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z!?.,\\' ]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64cac282-9a89-488c-b613-a002afd46485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'최고의 영화...!! 단연코 best 보는 내내 눈물 ㅠㅠ'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"亞 최고의 영화...!! 단연코 BEST 보는 내내 눈물 ㅠㅠ\"\n",
    "preprocess_text(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d222ef88-9931-4993-83a0-2166137cf2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wx/2vw1xwz972zdxff5h8gzrml00000gn/T/ipykernel_17935/1102241259.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"document\"] = train_df[\"document\"].apply(lambda x: preprocess_text(x))\n"
     ]
    }
   ],
   "source": [
    "train_df[\"document\"] = train_df[\"document\"].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcfb1b1-092c-417a-b159-2e180b237008",
   "metadata": {},
   "source": [
    "## huggingface tokenizers\n",
    "\n",
    "huggingface는 AI 스타트업으로 오픈 소스 라이브러리로 유명합니다. 특히 NLP 분야에서는 huggingface에서 제공하는 트랜스포머 모델을 사용하는 것이 거의 표준으로 자리잡았습니다. 주요 라이브러리는 아래와 같습니다. \n",
    "\n",
    "- transformers: 트랜스포머 기본 모델과 이를 응용한 NLP 모델들을 제공\n",
    "- tokenizers: subword tokenizer 제공\n",
    "\n",
    "huggingface의 tokenizers는 subword tokenizer의 구현체입니다. 이를 사용하여 tokenizer를 학습시켜 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4b24984-6965-47eb-a0a3-6e7d080ec5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212e38a-461e-46d7-8d6b-96d42dc4b9f3",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "WordPiece 기반의 subword tokenizer를 만들어보겠습니다.  먼저 tokenizer를 학습시키기 위한 trainer를 만들어줍니다. 이 때, special tokens를 넣어주어야 하는데, 각 토큰의 의미는 다음과 같습니다.\n",
    "\n",
    "- [PAD]: padding의 약자로 문장 간에 길이를 맞춰주기 위해 일부러 채워넣은 토큰을 의미합니다.\n",
    "- [UNK]: unknown의 약자로 인식하지 못한 토큰을 나타냅니다.\n",
    "- [SOS]: start of sentence의 약자로 문장의 시작을 표시해줍니다.\n",
    "- [EOS]: end of sentence의 약자로 문장의 끝을 표시해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3aa045-b350-431b-853a-2c96be125545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=10000,\n",
    "    min_frequency=50,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[SOS]\", \"[EOS]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838c82d-a95b-4b0c-a14f-375d3d307577",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "이제 tokenizer 객체를 만들어주고 trainer를 이용해서 학습시켜 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac97ea1b-14a2-4745-8bd0-b099c252d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(df, batch_size=1000):\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        yield batch_df[\"document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ab02590-4a50-4e43-aea5-2daae6101326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a819130-d37e-4b5d-b1fd-093b9b680bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(WordPiece())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.train_from_iterator(batch_iterator(train_df), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd1491-9711-4648-97f2-002325d64b7a",
   "metadata": {},
   "source": [
    "## tokenizer 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ede0e-15e0-4450-99c4-350410db649a",
   "metadata": {},
   "source": [
    "### vocab 확인\n",
    "\n",
    "먼저 tokenizer vocab에 어떤 토큰들이 추가되었는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bd8337d-ae68-458c-9277-b09b3428fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = tokenizer.get_vocab()\n",
    "sorted_vocabs = sorted(vocabs.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f5b96-dbb7-48d2-86b7-5add687a7f21",
   "metadata": {},
   "source": [
    "4000 ~ 5000개까지는 초기 기본 토큰들로 채워져 있고, 그 뒤로는 함께 자주 등장하는 글자끼리 묶인 토큰들을 확인할 수 있습니다. 앞에 ##이 붙은 토큰들은 단어의 시작 지점이 아닌 위치에 등장하는 토큰들입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf29bd92-2cbc-4d97-ad0b-bbdaafda9526",
   "metadata": {},
   "source": [
    "### 샘플 토큰화\n",
    "\n",
    "예시 문장들을 토큰화 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ba1d665-b5cb-4b33-8727-26bed6ffdb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    \"너무 재밌어요, 꿀잼 인정!\",\n",
    "    \"보다가 중간에 졸았습니다 ㅠㅠ 비추! 亞\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f87c73cd-2b47-4807-8700-42367db697ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4882, 5396, 6, 6020, 6309, 4]\n",
      "['너무', '재밌어요', ',', '꿀잼', '인정', '!']\n",
      "[5285, 5836, 1843, 6944, 5035, 6208, 4, 1]\n",
      "['보다가', '중간에', '졸', '##았습니다', 'ㅠㅠ', '비추', '!', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "for sample in samples:\n",
    "    output = tokenizer.encode(sample)\n",
    "    print(output.ids)\n",
    "    print(output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7804e2-c961-45db-b4d8-76d17d42efa5",
   "metadata": {},
   "source": [
    "### 저장\n",
    "\n",
    "잘 학습된 것을 확인했다면 파일에 저장하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27569947-c081-45e1-8986-86b9e07028cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"./data/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a5e3a-7625-4eb5-a9bd-5036ef7fb759",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이번 챕터에서는 huggingface에서 제공하는 tokenizers 라이브러리를 이용해서 직접 subword tokenizer를 만들어보았습니다. subword tokenizer는 활용도가 매우 높으니, 사용법을 잘 기억해주시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b650a6ea-064c-4a8b-9658-3cc0d9095174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
